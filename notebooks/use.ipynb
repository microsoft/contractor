{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is designed to process text files in a structured manner. It begins by loading the necessary data and defining agents and tools required for the task. These agents and tools are configured to work sequentially, ensuring that the text files located in the root directory are properly loaded. Once the text files are loaded, the script processes their content by resuming (extracting key points) from each file. Finally, it generates a concise summary of all the texts, providing a high-level overview of the information contained in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding src folder to path: c:\\Users\\ricar\\Github\\contractor\\src\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\"))\n",
    "print(\"Adding src folder to path:\", src_path)\n",
    "sys.path.insert(0, src_path)\n",
    "\n",
    "load_dotenv(f\"{src_path}/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models from the Source Folder\n",
    "\n",
    "To begin, ensure that the required models are loaded from the source folder. This step is crucial for initializing the necessary components for processing and analysis. Verify that the source folder contains all the relevant model files before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.schemas.models import VideoData, AudioData, ImageData, TextData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data Models\n",
    "\n",
    "In the next cell, we will load the data models corresponding to various data sources: audio, text, video, and image. These models represent the structured data from the respective folders inside the `./data` directory. Each model is designed to handle the specific characteristics of its data type, ensuring efficient processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioData instance: source='./data/audios/azure-podcast-2.mp3' objective='Explains Azure ACA and its features' tags=['azure', 'aca', 'containers'] encoding=None embeddings=None\n",
      "AudioData instance: source='./data/audios/azure-podcast-2.mp3' objective='Explains Azure in general' tags=['azure', 'definitions'] encoding=None embeddings=None\n"
     ]
    }
   ],
   "source": [
    "audio_data_1 = AudioData(\n",
    "    source=\"./data/audios/azure-podcast-2.mp3\",\n",
    "    objective=\"Explains Azure ACA and its features\",\n",
    "    tags=[\"azure\", \"aca\", \"containers\"]\n",
    ")\n",
    "\n",
    "audio_data_2 = AudioData(\n",
    "    source=\"./data/audios/azure-podcast-2.mp3\",\n",
    "    objective=\"Explains Azure in general\",\n",
    "    tags=[\"azure\", \"definitions\"]\n",
    ")\n",
    "\n",
    "print(\"AudioData instance:\", audio_data_1)\n",
    "print(\"AudioData instance:\", audio_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextData instance: source='./data/documents/aoai-assistants.pdf' objective='Explains Azure OpenAI and its features' tags=['azure', 'aoai', 'assistants'] encoding=None embeddings=None\n",
      "TextData instance: source='./data/documents/aoai-prompting.pdf' objective='Explains Prompting in Azure OpenAI' tags=['azure', 'aoai', 'prompting'] encoding=None embeddings=None\n"
     ]
    }
   ],
   "source": [
    "text_data_1 = TextData(\n",
    "    source=\"./data/documents/aoai-assistants.pdf\",\n",
    "    objective=\"Explains Azure OpenAI and its features\",\n",
    "    tags=[\"azure\", \"aoai\", \"assistants\"]\n",
    ")\n",
    "\n",
    "text_data_2 = TextData(\n",
    "    source=\"./data/documents/aoai-prompting.pdf\",\n",
    "    objective=\"Explains Prompting in Azure OpenAI\",\n",
    "    tags=[\"azure\", \"aoai\", \"prompting\"]\n",
    ")\n",
    "\n",
    "print(\"TextData instance:\", text_data_1)\n",
    "print(\"TextData instance:\", text_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Toolers and Creating the Assembly\n",
    "\n",
    "In this step, we focus on loading the appropriate toolers required for processing the data. Toolers are specialized components designed to handle specific tasks, such as extracting embeddings, encoding data, or performing analysis. Once the toolers are loaded, they are assembled into a cohesive workflow to ensure seamless processing of the data models (audio, video, image, and text).\n",
    "\n",
    "The assembly of toolers ensures that each data type is processed using the most suitable tools, leveraging their unique capabilities to extract meaningful insights and achieve the desired objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembly created with agents: id='67334a99-baac-416f-9b18-78ff78ef6612' objective='multimodal processing using local data for architecture review on azure' agents=[Agent(id='1f88b3b6-2e36-4bad-9088-0624668c628e', name='AudioAgent', model_id='default', metaprompt='This agent handles audio processing and extraction tasks.', objective='audio'), Agent(id='a2b31831-773e-40af-b925-9b5ecb8460ae', name='TextAgent', model_id='default', metaprompt='This agent analyzes textual information for semantic understanding.', objective='text')] roles=['audio', 'text']\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from app.schemas.models import Agent, Assembly\n",
    "\n",
    "audio_agent = Agent(\n",
    "    id=str(uuid.uuid4()),\n",
    "    name=\"AudioAgent\",\n",
    "    model_id=\"default\",\n",
    "    metaprompt=\"This agent handles audio processing and extraction tasks.\",\n",
    "    objective=\"audio\"\n",
    ")\n",
    "\n",
    "text_agent = Agent(\n",
    "    id=str(uuid.uuid4()),\n",
    "    name=\"TextAgent\",\n",
    "    model_id=\"default\",\n",
    "    metaprompt=\"This agent analyzes textual information for semantic understanding.\",\n",
    "    objective=\"text\"\n",
    ")\n",
    "\n",
    "assembly = Assembly(\n",
    "    id=str(uuid.uuid4()),\n",
    "    objective=\"multimodal processing using local data for architecture review on azure\",\n",
    "    agents=[audio_agent, text_agent],\n",
    "    roles=[\"audio\", \"text\"]\n",
    ")\n",
    "\n",
    "print(\"Assembly created with agents:\", assembly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.agents.main import ToolerOrchestrator\n",
    "\n",
    "orchestrator = ToolerOrchestrator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orchestrator response: Tools available:\n",
      "• Text extraction and summarization tool\n",
      "\n",
      "Selected tool: Text extraction and summarization tool\n",
      "\n",
      "Reference used:\n",
      "– Official Microsoft Azure documentation (Azure OpenAI, Azure Kubernetes Service (AKS), and Azure Container Apps (ACA))\n",
      "\n",
      "Answer:\n",
      "To build a chatbot using Azure OpenAI on either AKS or ACA, you essentially need to combine the power of Azure’s OpenAI service with a containerized application deployed to one of these managed container platforms. Here’s a high-level step‐by‐step guide outlining the process:\n",
      "\n",
      "1. Provision the Azure OpenAI Service:\n",
      " • Sign in to the Azure portal and create an Azure OpenAI resource. You’ll receive API keys and an endpoint URL that your chatbot backend will use to make calls to the language model.\n",
      " • Review usage limits and pricing details while ensuring that the specific OpenAI model(s) (like GPT-3.5 or GPT-4 variants) needed for your chatbot are enabled.\n",
      "\n",
      "2. Develop the Chatbot Application:\n",
      " • Create your chatbot’s backend using your preferred language (e.g., Python, Node.js, or .NET). The backend will receive user input, pass it as prompts to Azure OpenAI, and format the responses.\n",
      " • Implement secure access to the Azure OpenAI API by storing your keys safely (for example, in Azure Key Vault or as environment variables managed by secrets in your container orchestrator).\n",
      " • Optionally, build a frontend (web or mobile) that interacts with your backend.\n",
      " • Test locally by calling the Azure OpenAI API endpoints, verifying that your application receives the expected responses.\n",
      "\n",
      "3. Containerize the Chatbot Application:\n",
      " • Create a Dockerfile for your application. For example, a simple Python Dockerfile might look like:\n",
      "  ----------------------------------------------\n",
      "  FROM python:3.9-slim\n",
      "  WORKDIR /app\n",
      "  COPY requirements.txt .\n",
      "  RUN pip install -r requirements.txt\n",
      "  COPY . .\n",
      "  CMD [\"python\", \"app.py\"]\n",
      "  ----------------------------------------------\n",
      " • Build and test your container image locally before pushing it to a container registry (e.g., Azure Container Registry).\n",
      "\n",
      "4. Deploying to Azure Kubernetes Service (AKS) or Azure Container Apps (ACA):\n",
      "\n",
      " A. Using Azure Kubernetes Service (AKS):\n",
      "  • Provision an AKS cluster:\n",
      "   - Create a resource group:\n",
      "    az group create --name MyResourceGroup --location eastus\n",
      "   - Create your AKS cluster:\n",
      "    az aks create --resource-group MyResourceGroup --name myAKSCluster --node-count 1 --enable-addons monitoring --generate-ssh-keys\n",
      "   - Connect to your cluster:\n",
      "    az aks get-credentials --resource-group MyResourceGroup --name myAKSCluster\n",
      "  • Deploy your containerized chatbot:\n",
      "   - Create a Kubernetes Deployment and Service manifest (YAML file). For example, a simple deployment.yaml that describes your container image, environment variables (including your Azure OpenAI API key), and port configuration.\n",
      "   - Apply the configuration using:\n",
      "    kubectl apply -f deployment.yaml\n",
      "  • Optionally integrate ingress controllers, auto-scaling (using Horizontal Pod Autoscaler), and monitoring (via Azure Monitor).\n",
      "\n",
      " B. Using Azure Container Apps (ACA):\n",
      "  • ACA is a serverless container hosting platform that abstracts away much of the underlying orchestration.\n",
      "  • Create a Container Apps environment:\n",
      "   az containerapp env create --resource-group MyResourceGroup --name myEnv --location eastus\n",
      "  • Deploy your container:\n",
      "   az containerapp create \\\n",
      "    --name myChatbotApp \\\n",
      "    --resource-group MyResourceGroup \\\n",
      "    --environment myEnv \\\n",
      "    --image <your-container-registry>/myapp:latest \\\n",
      "    --target-port 80 \\\n",
      "    --ingress 'external' \\\n",
      "    --registry-username <username> --registry-password <password>\n",
      "  • Configure environment variables for the Azure OpenAI API key and endpoint if needed. ACA provides a simplified way of updating configurations and scaling (including scale-to-zero capabilities).\n",
      "\n",
      "5. Additional Considerations:\n",
      " • Security: Always secure your API keys and use HTTPS endpoints. Consider implementing authentication and rate limiting for your chatbot.\n",
      " • Scalability: Both AKS and ACA offer autoscaling, but ACA, being serverless, may offer greater simplicity for variable workloads.\n",
      " • Monitoring and Logging: Leverage Azure Monitor, Log Analytics, and Application Insights to track performance and diagnose issues.\n",
      " • Iteration and Testing: After deploying, conduct end-to-end tests to ensure the chatbot communicates correctly with Azure OpenAI and that scaling behaves as expected under load.\n",
      "\n",
      "By following these steps and referring to the official Microsoft Azure documentation for detailed configuration examples, you can build, containerize, and deploy a robust chatbot that leverages Azure OpenAI capabilities on either an AKS cluster or via the more serverless ACA option.\n",
      "\n",
      "Reference summary:\n",
      "– Microsoft’s official documentation provided guidance on Azure OpenAI connectivity, containerization best practices, and deployment procedures for both AKS and ACA.\n",
      "\n",
      "This guide should help you get started with your chatbot project using Azure OpenAI and your choice of container platforms.\n"
     ]
    }
   ],
   "source": [
    "response = await orchestrator.run_interaction(assembly=assembly, prompt=\"Explain how may I use Azure OpenAI to build a chatbot on AKS or ACA\", strategy=\"llm\")\n",
    "flattened = [str(item) for sublist in response for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "print(\"Orchestrator response:\", \"\\n\".join(flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orchestrator response: It seems that your request is empty or missing specific instructions. Please provide details about the audio content you want analyzed or edited, such as:\n",
      "\n",
      "- The audio files you'd like me to work on (e.g., filenames or folder location).\n",
      "- The specific request or changes you'd like made to the audio (e.g., transcription, speaker identification, embedding generation, noise reduction, edits, etc.).\n",
      "- The desired output or any additional information regarding the task.\n",
      "\n",
      "Once you provide this information, I'll proceed with analyzing and processing the audio accordingly!\n",
      "It appears no files or data could be retrieved from the specified directories `C:\\Users\\ricar\\Github\\augumented-rag\\notebook\\data\\documents` or `C:\\Users\\ricar\\Github\\augumented-rag\\notebook\\data\\audios`. Please verify the directory paths or contents to ensure the folders contain the expected files. Let me know how you'd like to proceed!\n"
     ]
    }
   ],
   "source": [
    "response = await orchestrator.run_interaction(\n",
    "    assembly=assembly,\n",
    "    prompt=\"\"\"\n",
    "        You have been given a few local documents and audios on the folder located at the directory 'C:\\\\Users\\\\ricar\\\\Github\\\\augumented-rag\\\\notebook\\\\data'\n",
    "        and contains the subfolders 'documents' and 'audios'. I need a abstract detailing their content.\n",
    "        Preciselly give the name of the Python Classes used as Tools on your answer.\n",
    "    \"\"\",\n",
    "    strategy=\"parallel\"\n",
    ")\n",
    "\n",
    "flattened = [\n",
    "    str(item)\n",
    "    for sublist in response\n",
    "    for item in (sublist if isinstance(sublist, list) else [sublist])\n",
    "]\n",
    "\n",
    "print(\"Orchestrator response:\", \"\\n\".join(flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = await orchestrator.run_interaction(assembly=assembly, prompt=\"Detail each information that I have to consider on Azure Container Instance to Run a multi-agentic RAG app\", strategy=\"llm\")\n",
    "#flattened = [str(item) for sublist in response for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "#print(\"Orchestrator response:\", \"\\n\".join(flattened))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
