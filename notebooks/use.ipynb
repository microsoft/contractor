{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is designed to process text files in a structured manner. It begins by loading the necessary data and defining agents and tools required for the task. These agents and tools are configured to work sequentially, ensuring that the text files located in the root directory are properly loaded. Once the text files are loaded, the script processes their content by resuming (extracting key points) from each file. Finally, it generates a concise summary of all the texts, providing a high-level overview of the information contained in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding src folder to path: c:\\Users\\ricar\\Github\\contractor\\src\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\"))\n",
    "print(\"Adding src folder to path:\", src_path)\n",
    "sys.path.insert(0, src_path)\n",
    "\n",
    "load_dotenv(f\"{src_path}/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models from the Source Folder\n",
    "\n",
    "To begin, ensure that the required models are loaded from the source folder. This step is crucial for initializing the necessary components for processing and analysis. Verify that the source folder contains all the relevant model files before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.schemas.models import VideoData, AudioData, ImageData, TextData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data Models\n",
    "\n",
    "In the next cell, we will load the data models corresponding to various data sources: audio, text, video, and image. These models represent the structured data from the respective folders inside the `./data` directory. Each model is designed to handle the specific characteristics of its data type, ensuring efficient processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioData instance: source='./data/audios/azure-podcast-2.mp3' objective='Explains Azure ACA and its features' tags=['azure', 'aca', 'containers'] encoding=None embeddings=None\n",
      "AudioData instance: source='./data/audios/azure-podcast-2.mp3' objective='Explains Azure in general' tags=['azure', 'definitions'] encoding=None embeddings=None\n"
     ]
    }
   ],
   "source": [
    "audio_data_1 = AudioData(\n",
    "    source=\"./data/audios/azure-podcast-2.mp3\",\n",
    "    objective=\"Explains Azure ACA and its features\",\n",
    "    tags=[\"azure\", \"aca\", \"containers\"]\n",
    ")\n",
    "\n",
    "audio_data_2 = AudioData(\n",
    "    source=\"./data/audios/azure-podcast-2.mp3\",\n",
    "    objective=\"Explains Azure in general\",\n",
    "    tags=[\"azure\", \"definitions\"]\n",
    ")\n",
    "\n",
    "print(\"AudioData instance:\", audio_data_1)\n",
    "print(\"AudioData instance:\", audio_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextData instance: source='./data/documents/aoai-assistants.pdf' objective='Explains Azure OpenAI and its features' tags=['azure', 'aoai', 'assistants'] encoding=None embeddings=None\n",
      "TextData instance: source='./data/documents/aoai-prompting.pdf' objective='Explains Prompting in Azure OpenAI' tags=['azure', 'aoai', 'prompting'] encoding=None embeddings=None\n"
     ]
    }
   ],
   "source": [
    "text_data_1 = TextData(\n",
    "    source=\"./data/documents/aoai-assistants.pdf\",\n",
    "    objective=\"Explains Azure OpenAI and its features\",\n",
    "    tags=[\"azure\", \"aoai\", \"assistants\"]\n",
    ")\n",
    "\n",
    "text_data_2 = TextData(\n",
    "    source=\"./data/documents/aoai-prompting.pdf\",\n",
    "    objective=\"Explains Prompting in Azure OpenAI\",\n",
    "    tags=[\"azure\", \"aoai\", \"prompting\"]\n",
    ")\n",
    "\n",
    "print(\"TextData instance:\", text_data_1)\n",
    "print(\"TextData instance:\", text_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Toolers and Creating the Assembly\n",
    "\n",
    "In this step, we focus on loading the appropriate toolers required for processing the data. Toolers are specialized components designed to handle specific tasks, such as extracting embeddings, encoding data, or performing analysis. Once the toolers are loaded, they are assembled into a cohesive workflow to ensure seamless processing of the data models (audio, video, image, and text).\n",
    "\n",
    "The assembly of toolers ensures that each data type is processed using the most suitable tools, leveraging their unique capabilities to extract meaningful insights and achieve the desired objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembly created with agents: id='0b41e66e-00b0-489b-b2e6-4e7da138b9f0' objective='multimodal processing using local data for architecture review on azure' agents=[Agent(id='33e13826-cc8c-4539-bf11-8d59751bb369', name='AudioAgent', model_id='default', metaprompt='This agent handles audio processing and extraction tasks.', objective='audio'), Agent(id='265ec049-5460-4c52-ae08-9931d06f57d3', name='TextAgent', model_id='default', metaprompt='This agent analyzes textual information for semantic understanding.', objective='text')] roles=['audio', 'text']\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from app.schemas.models import Agent, Assembly\n",
    "\n",
    "audio_agent = Agent(\n",
    "    id=str(uuid.uuid4()),\n",
    "    name=\"AudioAgent\",\n",
    "    model_id=\"default\",\n",
    "    metaprompt=\"This agent handles audio processing and extraction tasks.\",\n",
    "    objective=\"audio\"\n",
    ")\n",
    "\n",
    "text_agent = Agent(\n",
    "    id=str(uuid.uuid4()),\n",
    "    name=\"TextAgent\",\n",
    "    model_id=\"default\",\n",
    "    metaprompt=\"This agent analyzes textual information for semantic understanding.\",\n",
    "    objective=\"text\"\n",
    ")\n",
    "\n",
    "assembly = Assembly(\n",
    "    id=str(uuid.uuid4()),\n",
    "    objective=\"multimodal processing using local data for architecture review on azure\",\n",
    "    agents=[audio_agent, text_agent],\n",
    "    roles=[\"audio\", \"text\"]\n",
    ")\n",
    "\n",
    "print(\"Assembly created with agents:\", assembly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.agents.main import ToolerOrchestrator\n",
    "\n",
    "orchestrator = ToolerOrchestrator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orchestrator response: Available tools: \n",
      "• Text Extraction Tool – to extract and summarize content from text-based references.\n",
      "\n",
      "I’ll start by using the Text Extraction Tool to reference Microsoft’s official documentation on Azure OpenAI, AKS, and ACA. Although I don’t have a direct reference file attached here, I’m basing the overview on information from Microsoft Learn and related docs.\n",
      "\n",
      "Reference used: Microsoft Learn documentation for Azure OpenAI, Azure Kubernetes Service (AKS), and Azure Container Apps (ACA) (accessed via the Text Extraction Tool).\n",
      "\n",
      "Answer:\n",
      "You can build a chatbot that leverages Azure OpenAI by hosting your containerized application on either Azure Kubernetes Service (AKS) or Azure Container Apps (ACA). Below is an overview of the steps involved:\n",
      "\n",
      "1. Provision Azure OpenAI Service:\n",
      "   • In the Azure Portal, create an Azure OpenAI resource.\n",
      "   • Retrieve your API key and endpoint details, which your chatbot will use to send requests to the model (for example, a Chat Completions API).\n",
      "\n",
      "2. Develop Your Chatbot Application:\n",
      "   • Write your chatbot application’s code. For instance, you can create an API service in Python (using frameworks like FastAPI or Flask) or Node.js that takes user input, calls the Azure OpenAI API to generate responses, and returns them to the user.\n",
      "   • Handle conversation state and any additional logic (user authentication, session state, etc.).\n",
      "\n",
      "3. Containerize the Application:\n",
      "   • Create a Dockerfile for your application. This file will detail how to install dependencies and run your application.\n",
      "   • Test your container locally.\n",
      "\n",
      "4. Push the Container Image to a Registry:\n",
      "   • Use Azure Container Registry (ACR) or another container registry.\n",
      "   • Tag and push your Docker image to the registry so that it’s available for deployment.\n",
      "\n",
      "5. Deploy on AKS or ACA:\n",
      "   • For AKS (Azure Kubernetes Service):\n",
      "     - Set up an AKS cluster if you haven’t already.\n",
      "     - Create Kubernetes manifests (e.g., Deployment, Service, and optionally Ingress resources) that reference your container image.\n",
      "     - Inject environment variables or secrets (such as the Azure OpenAI API key and endpoint) into your pods.\n",
      "     - AKS gives you full control over orchestration, scaling, and load balancing.\n",
      "     \n",
      "   • For ACA (Azure Container Apps):\n",
      "     - ACA is a managed service that abstracts away much of the Kubernetes complexity.\n",
      "     - Use the Azure CLI or Portal to create a Container App, specifying your container image.\n",
      "     - Configure environment variables and scaling rules (for example, scale to zero when idle or based on HTTP request load).\n",
      "     - ACA integrates with other Azure services easily and can be easier to manage if you prefer a serverless-like experience.\n",
      "\n",
      "6. Integrate Azure OpenAI within Your Chatbot:\n",
      "   • Within your application’s code, use the retrieved endpoint and API key to send HTTP requests to the Azure OpenAI endpoint.\n",
      "   • Process replies from the OpenAI service and transform them into appropriate chatbot responses.\n",
      "   • Consider adding logging, error handling, and retry logic to manage network/API failures.\n",
      "\n",
      "7. Secure and Monitor Your Deployment:\n",
      "   • Use Azure Key Vault or Kubernetes secrets/ACA secrets to store sensitive API keys securely.\n",
      "   • Monitor your deployments using Azure Monitor or Log Analytics to track performance and troubleshoot issues.\n",
      "   • For production scenarios, set up secure communication (HTTPS) and proper identity management (managed identities) where possible.\n",
      "\n",
      "In summary, leveraging Azure OpenAI involves:\n",
      "• Creating and configuring your Azure OpenAI service.\n",
      "• Developing a chatbot service that uses this API.\n",
      "• Containerizing the service and deploying it to your preferred container platform (AKS for a controllable Kubernetes environment or ACA for a fully managed container experience).\n",
      "• Managing credentials, scaling, and monitoring to ensure a smooth production rollout.\n",
      "\n",
      "These steps collectively allow you to build a sophisticated, scalable chatbot using Azure’s advanced AI services alongside container orchestration platforms like AKS or ACA.\n",
      "\n",
      "Reference: Microsoft Learn documentation on Azure OpenAI, AKS, and ACA (information extracted via the Text Extraction Tool).\n"
     ]
    }
   ],
   "source": [
    "response = await orchestrator.run_interaction(assembly=assembly, prompt=\"Explain how may I use Azure OpenAI to build a chatbot on AKS or ACA\", strategy=\"llm\")\n",
    "flattened = [str(item) for sublist in response for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "print(\"Orchestrator response:\", \"\\n\".join(flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Erro durante a transcrição de áudio: Exception with error code: \n",
      "[CALL STACK BEGIN]\n",
      "\n",
      "    > pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - pal_string_to_wstring\n",
      "    - recognizer_create_speech_recognizer_from_config\n",
      "\n",
      "[CALL STACK END]\n",
      "\n",
      "Exception with an error code: 0x8 (SPXERR_FILE_OPEN_FAILED)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orchestrator response: It seems you've sent an empty prompt. Could you please provide more details or let me know how I can assist you?\n",
      "I encountered issues while processing the documents and audios:\n",
      "\n",
      "1. **Text Processing**: The summarization service is currently not configured, so I couldn't generate abstracts for the documents (`aoai-assistants.pdf`, `aoai-prompting.pdf`, `aoai.pdf`).\n",
      "\n",
      "2. **Audio Processing**: There was an error with the audio transcription due to a file access problem (`SPXERR_FILE_OPEN_FAILED`). Therefore, I couldn't extract details from the audio files.\n",
      "\n",
      "If you can reconfigure the services or provide alternate methods, please let me know so I can proceed effectively. Additionally, I could manually edit, review, or process any content based on further instructions.\n"
     ]
    }
   ],
   "source": [
    "response = await orchestrator.run_interaction(\n",
    "    assembly=assembly,\n",
    "    prompt=\"\"\"\n",
    "        You have been given a few local documents and audios on the folder located at the directory 'C:\\\\Users\\\\ricar\\\\Github\\\\augumented-rag\\\\notebook\\\\data'\n",
    "        and contains the subfolders 'documents' and 'audios'. I need a abstract detailing their content.\n",
    "        Preciselly give the name of the Python Classes used as Tools on your answer.\n",
    "    \"\"\",\n",
    "    strategy=\"parallel\"\n",
    ")\n",
    "\n",
    "flattened = [\n",
    "    str(item)\n",
    "    for sublist in response\n",
    "    for item in (sublist if isinstance(sublist, list) else [sublist])\n",
    "]\n",
    "\n",
    "print(\"Orchestrator response:\", \"\\n\".join(flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = await orchestrator.run_interaction(assembly=assembly, prompt=\"Detail each information that I have to consider on Azure Container Instance to Run a multi-agentic RAG app\", strategy=\"llm\")\n",
    "#flattened = [str(item) for sublist in response for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "#print(\"Orchestrator response:\", \"\\n\".join(flattened))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
